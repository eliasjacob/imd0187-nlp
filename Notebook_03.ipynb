{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "## IMD0187 - Natural Language Processing\n",
    "### [Dr. Elias Jacob de Menezes Neto](https://sigaa.ufrn.br/sigaa/public/docente/portal.jsf?siape=2353000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Keypoints\n",
    "\n",
    "- Recurrent Neural Networks (RNNs) process sequential data by maintaining hidden states across time steps, allowing them to capture dependencies and relationships between words in a sentence.\n",
    "\n",
    "- RNNs face challenges with vanishing and exploding gradients when dealing with long sequences. The vanishing gradient problem occurs when gradients become extremely small during backpropagation, preventing effective learning. The exploding gradient problem happens when gradients become very large, leading to unstable training.\n",
    "\n",
    "- Long Short-Term Memory (LSTM) networks address the limitations of RNNs by introducing a memory cell and gates (input, output, forget) that regulate information flow. This allows LSTMs to effectively capture long-term dependencies.\n",
    "\n",
    "- The LSTM architecture consists of an input gate that controls what new information is added to the cell state, a forget gate that determines what information to discard, an output gate that decides what information from the cell state is used to compute the hidden state, and a memory cell that maintains long-term information.\n",
    "\n",
    "- Bidirectional LSTM networks process sequences in both forward and backward directions, capturing both past and future contexts. This additional context helps improve the performance of sequence processing tasks.\n",
    "\n",
    "- Gated Recurrent Units (GRUs) simplify the LSTM architecture by combining the hidden and cell states into a single hidden state and using only two gates (update and reset). GRUs are computationally more efficient than LSTMs while still effectively handling long-term dependencies.\n",
    "\n",
    "- The case study demonstrated the application of RNN, LSTM, and GRU architectures on a name gender classification task using a dataset from the 2010 Brazilian Census. The Bidirectional LSTM achieved the highest accuracy, followed by the Bidirectional GRU, LSTM, and Vanilla RNN.\n",
    "\n",
    "### Takeaways\n",
    "\n",
    "- Understanding RNNs, LSTMs, and GRUs is crucial for natural language processing tasks that involve sequential data and require capturing dependencies over time.\n",
    "\n",
    "- The choice of architecture depends on the specific task and its complexity. RNNs can suffice for simpler, short-term dependencies, while LSTMs and GRUs are better suited for more complex tasks or longer sequences.\n",
    "\n",
    "- Bidirectional networks offer additional context by processing sequences in both forward and backward directions, leading to improved performance in sequence processing tasks.\n",
    "\n",
    "- Experimenting with different architectures, hyperparameters, and iterative model evaluation is essential to find the optimal solution for a given task.\n",
    "\n",
    "- Comprehending the strengths and limitations of each architecture and considering the tradeoffs between model complexity, computational efficiency, and performance is crucial when selecting the appropriate model.\n",
    "\n",
    "- Mastering these foundational concepts and techniques provides a strong basis for advancing skills in natural language processing and tackling more complex tasks such as sentiment analysis, machine translation, and text generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Side Note: This class contains a set of very vanilla implementations with tools that we don't usually use in production. For example, we use the `torchtext` library to load the dataset and create the vocabulary. In a real-world scenario, we would use a more efficient and scalable approach, such as tokenization with `transformers` or `spaCy` and data loading with `DataLoader` from PyTorch. Please bear in mind that the goal of this class is to provide a basic understanding of RNNs, LSTMs, and GRUs, not to showcase the most efficient implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Networks for NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "When processing textual data, it's crucial to consider the dependencies and relationships between words in a sentence. The semantics of a sentence can change profoundly based on the order and selection of words.\n",
    "\n",
    "Consider these two similar sentences:\n",
    "> \"A bomba explodiu no jornal.\"\n",
    ">\n",
    "> \"A notícia do jornal explodiu como uma bomba\"\n",
    "\n",
    "Despite having analogous structure, interchanging just one adjective leads to a completely different meaning and emotional impact on the reader. Context plays a vital role, especially when a sentence's overall meaning can be greatly influenced by what has been said or happened previously.\n",
    "\n",
    "*Recurrent Neural Networks* (RNNs) provide neural networks with the capability to memorize previous words within a statement, enabling them to better capture and understand patterns that appear when certain tokens appear sequentially relative to other tokens. This is the fundamental premise of RNNs.\n",
    "\n",
    "## How RNNs Maintain State Across Time\n",
    "\n",
    "RNNs operate on the principle of maintaining state across time. While initially, it might seem complicated, it's essentially about giving the network a context for its current operation based on historical data.\n",
    "\n",
    "For each input fed into a standard feed-forward network, the output from one time step 't' is provided as an additional input for the next step 't+1', along with the fresh data being supplied at 't+1'. In simpler terms, you inform the network about what happened earlier alongside what is happening \"now\".\n",
    "\n",
    "This concept forms the basis of RNNs—which learn and remember over time, enabling them to better capture patterns within sequences. Understanding this is key to exploiting the power of RNNs for text analysis and other sequential data processing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing a RNN\n",
    "\n",
    "You can visualize a recurrent net as shown in figure below:\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"images/rnn_unrolled.png\" alt=\"\" style=\"width: 50%; height: 50%\"/>\n",
    "</p>\n",
    "\n",
    "\n",
    "Look at the left side. The circles are entire feedforward network layers composed of one or more neurons. The output of the hidden layer emerges from the network as normal, but it’s also set aside to be passed back in as an input to itself along with the normal input from the next time step. This feedback is represented with an arc from the output of a layer back into its own input.\n",
    "\n",
    "An easier way to see this process—and it’s more commonly shown this way—is by unrolling the net. The right side of the image above shows the network stood on its head with two unfoldings of the time variable (t), showing layers for t+1 and t+2.\n",
    "\n",
    "Each time step is represented by a column of neurons in the unrolled version of the very same neural network. It’s like looking at a screenplay or video frame of the neural net for each sample in time. The network to the right is the future version of the network on the left. The output of a hidden layer at one time step (t) is fed back into the hidden layer along with input data for the next time step (t+1) to the right. Repeat. This diagram shows two iterations of this unfolding, so three columns of neurons for t=0, t=1, and t=2.\n",
    "\n",
    "All the vertical paths in this visualization are clones, or views of the same neurons. They are the single network represented on a timeline. This visualization is helpful when talking about how information flows through the network forward and backward during backpropagation. But when looking at the three unfolded networks, remember that they’re all different snapshots of the same network with a single set of weights.\n",
    "\n",
    "### Structure of RNN: Feedforward Network Layers\n",
    "\n",
    "Viewing the left side of the image above, you'll notice circles that represent layers in a feedforward network, with each layer comprising one or more neurons. The output of the hidden layer not only moves forward through the network but also feeds back into the input of its originating layer.\n",
    "\n",
    "This recurrent feedback is illustrated by an arc looping from the layer's output back to its own input.\n",
    "\n",
    "### Unfolding Time Variable for Better Visualization\n",
    "\n",
    "To better visualize this process, we can 'unroll' the network over time. This technique, represented on the right side of the image, essentially flips the network on its head, revealing the progress of the network over two stages of the time variable (t), namely t+1 and t+2.\n",
    "\n",
    "Each time step 't' is denoted as a column of neurons in the unrolled version of our network. It can be thought of as watching successive frames of a movie, where each frame represents the state of the network at a given moment in time.\n",
    "\n",
    "### Cloned Views of Same Neurons\n",
    "\n",
    "In this representation, all vertical paths are clones or different views of the same set of neurons; they depict the same neural network captured at various points along a timeline.\n",
    "\n",
    "While this kind of representation simplifies comprehension of information flow (both forward and backward during backpropagation), it's essential to remember when looking at these multiple 'unfolded' networks: they are merely simultaneous snapshots of the same single network maintaining a consistent set of weights.\n",
    "\n",
    "\n",
    "> Recognizing an unrolled RNN as sequential instances of the same network operating over time is crucial for understanding how RNNs capture and utilize temporal information from sequences. This comprehension forms the basis of effectively utilizing the power of RNNs for sequence data processing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our first RNN\n",
    "\n",
    "### Our case study\n",
    "\n",
    "Let's discuss an interesting scenario. Assume you are employed at the Ombudsman Office of our University. A major part of your role involves addressing students' complaints and correspondingly communicating with the related departments. As a measure to enhance the quality of your communication, you have decided to use gender-appropriate pronouns based on the person's first name.\n",
    "\n",
    "One important goal is to avoid incorrectly gendering specific roles or offices within the university (e.g., naming the President's office as \"Gabinete do Reitor\" even when the President is a female, which happened between 2011 and 2019 at UFRN).\n",
    "\n",
    "To achieve this objective, we will be using a [dataset collected by IBGE during 2010 Census](https://brasil.io/dataset/genero-nomes/nomes/). This dataset contains a total of 90,104 names, out of which 49,274 are female and 40,830 are male. To ensure accuracy, any names that could be associated with both genders, such as \"Elias\", \"Ivani\" or \"Alison\", have been excluded from our analysis.\n",
    "\n",
    "> Interestingly, during my data exploration, I found out that there were 189,315 men and 1,387 women with the same name as mine. I'd never imagine women could be named Elias!\n",
    "\n",
    "Our aim here is to develop a Recurrent Neural Network (RNN) that can read a name letter by letter, and predict the probability of the name being either masculine or feminine. For the purpose of this project:\n",
    "\n",
    "- Each lowercase letter will be considered a token\n",
    "- The vocabulary will comprise the 26 alphabet letters\n",
    "- Any accented letters will be converted to their non-accented versions\n",
    "\n",
    "We will divide our dataset into two parts: 80% for training and 20% for validation.\n",
    "\n",
    "> Please note that while we recognize and respect the existence of non-binary gender identities, for the purposes of this exercise, we will be employing a binary classification model due to dataset limitations. Our dataset solely contains names which are classified as either masculine or feminine, hence we are confined to two classes. Maybe in the future we can work on a more inclusive model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that said, let's start by importing the necessary libraries and loading our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the unicodedata library for Unicode character database\n",
    "import unicodedata\n",
    "\n",
    "# Import PyTorch library for deep learning\n",
    "import torch\n",
    "\n",
    "# Import neural network module from PyTorch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Import functional interface for neural networks from PyTorch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Import optimization algorithms from PyTorch\n",
    "import torch.optim as optim\n",
    "\n",
    "# Import data module from torchtext for handling text data\n",
    "# Note: You may need to install this library at version 0.6.0 using pip install torchtext==0.6.0\n",
    "from torchtext import data\n",
    "\n",
    "# Import torchtext library for text processing\n",
    "import torchtext\n",
    "\n",
    "# Import pandas library for data manipulation and analysis\n",
    "import pandas as pd\n",
    "\n",
    "# Import random module for generating random numbers\n",
    "import random\n",
    "\n",
    "# Import accuracy_score, classification_report, and confusion_matrix from sklearn for evaluation metrics\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Import datetime and timedelta from datetime module for handling date and time\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>label</th>\n",
       "      <th>label_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>silmari</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jovanilde</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yorrana</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nakita</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tiarle</td>\n",
       "      <td>0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90098</th>\n",
       "      <td>edsmar</td>\n",
       "      <td>0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90099</th>\n",
       "      <td>altenice</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90100</th>\n",
       "      <td>arthemis</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90101</th>\n",
       "      <td>mielly</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90102</th>\n",
       "      <td>geasi</td>\n",
       "      <td>0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90103 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            name  label label_str\n",
       "0        silmari      1         F\n",
       "1      jovanilde      1         F\n",
       "2        yorrana      1         F\n",
       "3         nakita      1         F\n",
       "4         tiarle      0         M\n",
       "...          ...    ...       ...\n",
       "90098     edsmar      0         M\n",
       "90099   altenice      1         F\n",
       "90100   arthemis      1         F\n",
       "90101     mielly      1         F\n",
       "90102      geasi      0         M\n",
       "\n",
       "[90103 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/names_gender.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jose\n",
      "cafe\n",
      "uber\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "def normalize_name(name):\n",
    "    # Step 1: Normalize the Unicode string\n",
    "    # NFKD stands for Normalization Form Compatibility Decomposition\n",
    "    # This step separates combined characters into base character and diacritical marks\n",
    "    normalized = unicodedata.normalize('NFKD', name)\n",
    "    \n",
    "    # Step 2: Encode to ASCII and ignore non-ASCII characters\n",
    "    # This effectively removes accents and other diacritical marks\n",
    "    ascii_encoded = normalized.encode('ASCII', 'ignore')\n",
    "    \n",
    "    # Step 3: Decode back to UTF-8\n",
    "    # This converts the bytes object back into a string\n",
    "    utf8_decoded = ascii_encoded.decode('utf-8')\n",
    "    \n",
    "    # Step 4: Convert to lowercase\n",
    "    # This ensures consistent casing across all names\n",
    "    lowercased = utf8_decoded.lower()\n",
    "    \n",
    "    return lowercased\n",
    "\n",
    "# Example usage:\n",
    "# This will output 'jose', removing the accent from the 'é'\n",
    "print(normalize_name('José'))  # Output: jose \n",
    "print(normalize_name('Café'))  # Output: cafe\n",
    "print(normalize_name('Über'))  # Output: uber\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>label</th>\n",
       "      <th>label_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>silmari</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jovanilde</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yorrana</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nakita</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tiarle</td>\n",
       "      <td>0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90098</th>\n",
       "      <td>edsmar</td>\n",
       "      <td>0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90099</th>\n",
       "      <td>altenice</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90100</th>\n",
       "      <td>arthemis</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90101</th>\n",
       "      <td>mielly</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90102</th>\n",
       "      <td>geasi</td>\n",
       "      <td>0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90103 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            name  label label_str\n",
       "0        silmari      1         F\n",
       "1      jovanilde      1         F\n",
       "2        yorrana      1         F\n",
       "3         nakita      1         F\n",
       "4         tiarle      0         M\n",
       "...          ...    ...       ...\n",
       "90098     edsmar      0         M\n",
       "90099   altenice      1         F\n",
       "90100   arthemis      1         F\n",
       "90101     mielly      1         F\n",
       "90102      geasi      0         M\n",
       "\n",
       "[90103 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90103, 3)\n",
      "(90103, 3)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df['name'] = df['name'].apply(normalize_name)\n",
    "df.drop_duplicates(inplace=True, subset=['name'], keep=False)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>label</th>\n",
       "      <th>label_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>58546</th>\n",
       "      <td>jose</td>\n",
       "      <td>0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       name  label label_str\n",
       "58546  jose      0         M"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.query('name == \"jose\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>label</th>\n",
       "      <th>label_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30451</th>\n",
       "      <td>maria</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        name  label label_str\n",
       "30451  maria      1         F"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.query('name == \"maria\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['j', 'o', 's', 'e']\n"
     ]
    }
   ],
   "source": [
    "# This will be our simple tokenizer. It will split the names into letters\n",
    "\n",
    "def custom_tokenizer_letters(text):\n",
    "    text = normalize_name(text)\n",
    "    \n",
    "    # Convert the normalized text into a list of individual characters\n",
    "    # This approach treats each letter as a separate token\n",
    "    return list(text)\n",
    "\n",
    "# Example usage of the tokenizer\n",
    "print(custom_tokenizer_letters('José'))\n",
    "\n",
    "# Expected output: ['j', 'o', 's', 'e']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the LabelField and Field classes from the torchtext.data module\n",
    "from torchtext import data\n",
    "\n",
    "# Define a LabelField to represent the gender label for each name\n",
    "GENDER = data.LabelField()\n",
    "\n",
    "# Define a Field to represent the name text, using our custom tokenizer and including the length of each name\n",
    "NAME = data.Field(tokenize=custom_tokenizer_letters, lower=True, include_lengths=True)\n",
    "\n",
    "# Create a list of tuples representing the fields in our dataset, with the name field first and the gender label field second\n",
    "fields = [('name', NAME), ('label', GENDER)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the TabularDataset class from the torchtext.data module\n",
    "from torchtext.data import TabularDataset\n",
    "\n",
    "# Create a TabularDataset from our CSV file, using the fields we defined earlier\n",
    "dset = TabularDataset(path='data/names_gender.csv', format='CSV', fields=fields, skip_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and validation sets, with a split ratio of 80/20\n",
    "# The split is stratified based on the gender label, so that each set has roughly the same proportion of male and female names\n",
    "# The random seed is set to ensure reproducibility\n",
    "(train_dataset, valid_dataset) = dset.split(split_ratio=[0.8, 0.2], stratified=True, strata_field='label', random_state=random.seed(271828))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store the name and label tuples\n",
    "names_valid = list()\n",
    "\n",
    "# Iterate over each example in the validation dataset\n",
    "for ex in valid_dataset.examples:\n",
    "    # Join the list of letters in the name field to create a single string\n",
    "    n = ''.join(ex.name)\n",
    "    # Get the label field value (either 0 or 1)\n",
    "    l = ex.label\n",
    "    # Append a tuple of the name and label to the list\n",
    "    names_valid.append((n, l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('vannia', '1'),\n",
       " ('taiele', '1'),\n",
       " ('dicineia', '1'),\n",
       " ('jemyson', '0'),\n",
       " ('mrizete', '1'),\n",
       " ('vanuzia', '1'),\n",
       " ('larinha', '1'),\n",
       " ('luiton', '0'),\n",
       " ('eiiti', '0'),\n",
       " ('aldemberg', '0')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choices(names_valid, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store the name and label tuples\n",
    "names_train = list()\n",
    "\n",
    "# Iterate over each example in the training dataset\n",
    "for ex in train_dataset.examples:\n",
    "    # Join the list of letters in the name field to create a single string\n",
    "    n = ''.join(ex.name)\n",
    "    # Get the label field value (either 0 or 1)\n",
    "    l = ex.label\n",
    "    # Append a tuple of the name and label to the list\n",
    "    names_train.append((n, l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ermson', '0'),\n",
       " ('ystefane', '1'),\n",
       " ('dianina', '1'),\n",
       " ('euricelia', '1'),\n",
       " ('edneli', '1'),\n",
       " ('erigleidson', '0'),\n",
       " ('quenedy', '0'),\n",
       " ('kallinca', '1'),\n",
       " ('angerlandia', '1'),\n",
       " ('rimer', '0')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choices(names_train, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'1': 39419, '0': 32663})\n",
      "Counter({'1': 9855, '0': 8166})\n"
     ]
    }
   ],
   "source": [
    "# Import the Counter class from the collections module\n",
    "from collections import Counter\n",
    "\n",
    "# Use a list comprehension to extract the gender labels from the training and validation datasets\n",
    "# The Counter class is then used to count the frequency of each label (0 for male, 1 for female)\n",
    "# The resulting counts are printed to the console\n",
    "print(Counter([ex.label for ex in train_dataset.examples]))\n",
    "print(Counter([ex.label for ex in valid_dataset.examples]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72082, 18021)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the maximum size for the vocabulary\n",
    "vocab_size = 50\n",
    "\n",
    "# Build the vocabulary for the 'NAME' field using the training dataset\n",
    "# Limit the vocabulary size to the specified maximum\n",
    "NAME.build_vocab(train_dataset, max_size=vocab_size)\n",
    "\n",
    "# Build the vocabulary for the 'GENDER' field using the training dataset\n",
    "# No size limit is specified for this vocabulary\n",
    "GENDER.build_vocab(train_dataset)\n",
    "\n",
    "# Get the length of the vocabulary for the 'NAME' field\n",
    "len(NAME.vocab) # 26 letters + 1 for unknown + 1 for padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 71152), ('i', 66096), ('e', 63120), ('l', 47180), ('n', 46335)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NAME.vocab.freqs.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(None, {'1': 0, '0': 1})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GENDER.vocab.stoi # stoi: string to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 <unk>\n",
      "1 <pad>\n",
      "2 a\n",
      "3 i\n",
      "4 e\n",
      "5 l\n",
      "6 n\n",
      "7 r\n",
      "8 o\n",
      "9 d\n",
      "10 s\n",
      "11 c\n",
      "12 m\n",
      "13 t\n",
      "14 u\n",
      "15 v\n",
      "16 j\n",
      "17 y\n",
      "18 g\n",
      "19 h\n",
      "20 z\n",
      "21 b\n",
      "22 k\n",
      "23 f\n",
      "24 w\n",
      "25 p\n",
      "26 q\n",
      "27 x\n"
     ]
    }
   ],
   "source": [
    "# Let's check our vocabulary\n",
    "for i in range(len(NAME.vocab)):\n",
    "    print(i, NAME.vocab.itos[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if a GPU is available for PyTorch\n",
    "# torch.cuda.is_available() returns True if a GPU is available, otherwise False\n",
    "# If a GPU is available, set the device to 'cuda' to utilize the GPU for computations\n",
    "# If a GPU is not available, set the device to 'cpu' to use the CPU for computations\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the BucketIterator class from the torchtext.data module to create iterators for the training and validation datasets\n",
    "# The iterators will be used to generate batches of data during training and validation\n",
    "# The batch size is set to 32, and the device is set to the specified device (e.g. 'cpu' or 'cuda')\n",
    "# The sort_key argument specifies the function to use for sorting examples within each batch (in this case, the length of the name field)\n",
    "# The sort_within_batch argument specifies whether to sort examples within each batch (in this case, True)\n",
    "\n",
    "train_iter, valid_iter = data.BucketIterator.splits((train_dataset, valid_dataset),\n",
    "                                                  batch_size = 32,\n",
    "                                                  device = device,\n",
    "                                                  sort_key = lambda x: len(x.name),\n",
    "                                                  sort_within_batch = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Define a class for our RNN model\n",
    "class NameRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding_dim, vocab_size, pad_idx):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define an embedding layer to convert our letters to vectors\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        \n",
    "        # Define an RNN layer to encode our sequence of letters\n",
    "        self.encoder = nn.RNN(\n",
    "            input_size=embedding_dim,  # Size of each input vector\n",
    "            hidden_size=hidden_size,   # Number of features in the hidden state\n",
    "            num_layers=2,              # Number of recurrent layers\n",
    "            dropout=0.3,               # Dropout probability for the RNN layers\n",
    "            bidirectional=False        # Whether the RNN is bidirectional\n",
    "        )\n",
    "\n",
    "        # Define a dropout layer to prevent overfitting\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        # Define a classifier layer to output our final prediction\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2), # First linear layer\n",
    "            self.dropout,                             # Dropout layer\n",
    "            nn.ReLU(),                                # Activation function\n",
    "            nn.Linear(hidden_size // 2, 2)            # Second linear layer, output size is 2 for binary classification\n",
    "        )\n",
    "\n",
    "    def forward(self, seq, name_len):\n",
    "        # Convert our sequence of letters to a sequence of vectors using the embedding layer\n",
    "        embedded = self.embedding(seq)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        # Encode the embedded sequence using the RNN layer\n",
    "        packed_output, hidden = self.encoder(embedded)\n",
    "        \n",
    "        # Pass the final hidden state through the classifier layer to get our prediction\n",
    "        preds = self.classifier(hidden[-1])\n",
    "        \n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_idx = NAME.vocab.stoi[NAME.pad_token]\n",
    "pad_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn = NameRNN(\n",
    "    hidden_size=50,            # Hidden state size of the RNN\n",
    "    embedding_dim=25,          # Dimension of the embedding vectors\n",
    "    vocab_size=len(NAME.vocab),# Vocabulary size based on the 'NAME' field\n",
    "    pad_idx=pad_idx            # Padding index for the embedding layer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example, we've selected an arbitrary hidden size of 50 and an embedding dimension of 25. These choices were made largely to minimize computation time; however, adjusting these parameters can yield different results in both computational speed and model accuracy. Hence, it's encouraged that you experiment with varying these numbers based on your specific use-cases.\n",
    "\n",
    "#### Rule of Thumb for Model Complexity\n",
    "\n",
    "A helpful guiding principle when designing models is to ensure that the complexity of your model aligns appropriately with your data's innate structure. The objective is to achieve a balance where:\n",
    "\n",
    "1. Your model isn't too complex for your data (Overfitting), and\n",
    "2. It's not too simple relative to your data (Underfitting).\n",
    "\n",
    "Let's understand what this means:\n",
    "\n",
    "##### Overfitting - High Variance and Low Bias\n",
    "\n",
    "When a model is overly complex, it tends to \"memorize\" the training data rather than \"learning\" from it, causing poor generalization when faced with new, unseen data. This scenario is referred to as **overfitting** the data and results in a model with high variance and low bias.\n",
    "\n",
    "##### Underfitting - Low Variance and High Bias\n",
    "\n",
    "Conversely, if a model is too uncomplicated, it will fail even in capturing the fundamental patterns of the training data. Such underutilizing models are prone to consistently generate inaccurate predictions across all data types, both seen and unseen. This sub-optimal situation is known as **underfitting** and leads to a model with low variance and high bias.\n",
    "\n",
    "#### Balancing Between Bias and Variance\n",
    "\n",
    "Balancing between overfitting and underfitting is often described as managing the trade-off between bias and variance. The key is to find a sweet spot where the model is just complex enough to learn useful patterns from the training data but also retains the ability to generalize effectively to unseen data.\n",
    "\n",
    "While building your model, it's essential to keep this concept in mind: experiment with different parameters, monitor how they affect the performance of your model, and fine-tune them for optimal results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 10,977 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# Function to count the number of trainable parameters in a model\n",
    "def count_parameters(model):\n",
    "    # Sum the number of elements (numel) for each parameter in the model\n",
    "    # Only include parameters that require gradients (i.e., are trainable)\n",
    "    n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    # Print the total number of trainable parameters in a human-readable format\n",
    "    print(f\"The model has {n_parameters:,} trainable parameters\")\n",
    "    \n",
    "    # Return the total number of trainable parameters\n",
    "    return n_parameters\n",
    "\n",
    "# Count the number of trainable parameters in the model_rnn instance\n",
    "n_parameters = count_parameters(model_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training function\n",
    "def train(epochs, model, optimizer, criterion, train_iterator, valid_iterator, device, checkpoint_fname, verbose=True):\n",
    "    if verbose:\n",
    "        # Print the number of trainable parameters\n",
    "        n_parameters = count_parameters(model)\n",
    "\n",
    "    # Set a timer to measure training duration\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    # Move the model and loss function to the specified device (CPU or GPU)\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    # Initialize the best validation loss to infinity\n",
    "    best_valid_loss = float('inf')\n",
    "    \n",
    "    # Loop over the specified number of epochs\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        \n",
    "        # Initialize the training loss for this epoch\n",
    "        training_loss = 0.0\n",
    "        \n",
    "        # Set the model to training mode\n",
    "        model.train()\n",
    "        \n",
    "        # Loop over the training data in batches\n",
    "        for batch_idx, batch in enumerate(train_iterator):\n",
    "            # Zero the gradients to prevent accumulation\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Get the name and name length from the batch\n",
    "            nome, name_len = batch.name\n",
    "            \n",
    "            # Make a prediction using the model and calculate the loss\n",
    "            predict = model(nome, name_len).squeeze(1)\n",
    "            loss = criterion(predict, batch.label)\n",
    "            \n",
    "            # Backpropagate the loss and update the model parameters\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Add the batch loss to the total training loss\n",
    "            training_loss += loss.data.item() * batch.name[0].size(0)\n",
    "        \n",
    "        # Calculate the average training loss for this epoch\n",
    "        training_loss /= len(train_iterator)\n",
    "        \n",
    "        # Set the model to evaluation mode\n",
    "        model.eval()\n",
    "        \n",
    "        # Initialize the validation loss for this epoch\n",
    "        valid_loss = 0.0\n",
    "        \n",
    "        # Loop over the validation data in batches\n",
    "        for batch_idx, batch in enumerate(valid_iterator):\n",
    "            # Get the name and name length from the batch\n",
    "            nome, name_len = batch.name\n",
    "            \n",
    "            # Make a prediction using the model and calculate the loss\n",
    "            predict = model(nome, name_len).squeeze(1)\n",
    "            loss = criterion(predict, batch.label)\n",
    "            \n",
    "            # Add the batch loss to the total validation loss\n",
    "            valid_loss += loss.data.item() * batch.name[0].size(0)\n",
    "        \n",
    "        # Calculate the average validation loss for this epoch\n",
    "        valid_loss /= len(valid_iterator)\n",
    "        \n",
    "        # If the validation loss is better than the previous best, save the model and print a message\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), checkpoint_fname)\n",
    "            if verbose:\n",
    "                print(f'Epoch {epoch} - Training Loss: {training_loss:.4f} - Valid Loss: {valid_loss:.4f} - New Best')\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(f'Epoch {epoch} - Training Loss: {training_loss:.4f} - Valid Loss: {valid_loss:.4f}')\n",
    "    \n",
    "    if verbose:\n",
    "        # Print the total time elapsed and the mean time per epoch\n",
    "        elapsed_time = datetime.now() - start_time\n",
    "        print(f'Time elapsed: {elapsed_time}')\n",
    "        print(f'Mean time per epoch: {elapsed_time / epochs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Define a function to predict the gender label for a given name using a trained model\n",
    "# The function takes three arguments:\n",
    "# - nome: the name to predict the label for\n",
    "# - model: the trained model to use for prediction\n",
    "# - device: the device to use for computation (default is 'cpu')\n",
    "def predict(nome, model, device='cpu'):\n",
    "    # Preprocess the name using the NAME field's preprocess method and get its length\n",
    "    # The NAME.process method converts the name into a tensor and calculates its length\n",
    "    name, name_len = NAME.process([NAME.preprocess(nome)])\n",
    "    \n",
    "    # Move the preprocessed name and its length to the specified device (CPU or GPU)\n",
    "    name = name.to(device)\n",
    "    name_len = name_len.to(device)\n",
    "    \n",
    "    # Pass the preprocessed name and its length through the model to get the logits (unnormalized scores) for each label\n",
    "    logits = model(name, name_len)\n",
    "    \n",
    "    # Apply the softmax function to the logits to get the predicted probabilities for each label\n",
    "    result = F.softmax(logits, dim=1)\n",
    "    \n",
    "    # Create a dictionary to map the label indices to their corresponding gender labels ('0' -> 'M', '1' -> 'F')\n",
    "    result_dict = {'1': 'F', '0': 'M'}\n",
    "    \n",
    "    # Get the index of the label with the highest predicted probability\n",
    "    # Use the result dictionary to map the index to its corresponding gender label\n",
    "    label = GENDER.vocab.itos[result.argmax().item()]\n",
    "    label = result_dict[label]\n",
    "    \n",
    "    # Return a list containing the predicted gender label and its corresponding probability\n",
    "    return [label, result.max().item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Path class from the pathlib module to handle file system paths\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the path where the model checkpoints will be saved\n",
    "checkpoint_path = Path('./outputs/rnns/')\n",
    "\n",
    "# Create the directory (and any necessary parent directories) if it doesn't already exist\n",
    "checkpoint_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define the optimizer to use for training the model\n",
    "# Here, we use the Adam optimizer with a learning rate of 3e-4\n",
    "optimizer = optim.Adam(model_rnn.parameters(), lr=3e-4)\n",
    "\n",
    "# Define the loss function to use for training the model\n",
    "# Here, we use the CrossEntropyLoss, which is suitable for classification tasks\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 10,977 trainable parameters\n",
      "Epoch 1 - Training Loss: 2.1690 - Valid Loss: 1.4515 - New Best\n",
      "Epoch 2 - Training Loss: 1.6889 - Valid Loss: 1.2581 - New Best\n",
      "Epoch 3 - Training Loss: 1.5247 - Valid Loss: 1.1709 - New Best\n",
      "Epoch 4 - Training Loss: 1.4318 - Valid Loss: 1.1036 - New Best\n",
      "Epoch 5 - Training Loss: 1.3322 - Valid Loss: 1.0420 - New Best\n",
      "Epoch 6 - Training Loss: 1.2609 - Valid Loss: 0.9848 - New Best\n",
      "Epoch 7 - Training Loss: 1.2197 - Valid Loss: 0.9557 - New Best\n",
      "Epoch 8 - Training Loss: 1.1665 - Valid Loss: 0.9519 - New Best\n",
      "Epoch 9 - Training Loss: 1.1393 - Valid Loss: 0.9095 - New Best\n",
      "Epoch 10 - Training Loss: 1.1084 - Valid Loss: 0.9200\n",
      "Epoch 11 - Training Loss: 1.0781 - Valid Loss: 0.8862 - New Best\n",
      "Epoch 12 - Training Loss: 1.0588 - Valid Loss: 0.9175\n",
      "Epoch 13 - Training Loss: 1.0438 - Valid Loss: 0.8816 - New Best\n",
      "Epoch 14 - Training Loss: 1.0230 - Valid Loss: 0.8635 - New Best\n",
      "Epoch 15 - Training Loss: 1.0127 - Valid Loss: 0.8467 - New Best\n",
      "Epoch 16 - Training Loss: 0.9938 - Valid Loss: 0.8150 - New Best\n",
      "Epoch 17 - Training Loss: 0.9805 - Valid Loss: 0.8618\n",
      "Epoch 18 - Training Loss: 0.9664 - Valid Loss: 0.8130 - New Best\n",
      "Epoch 19 - Training Loss: 0.9618 - Valid Loss: 0.8057 - New Best\n",
      "Epoch 20 - Training Loss: 0.9485 - Valid Loss: 0.8121\n",
      "Epoch 21 - Training Loss: 0.9431 - Valid Loss: 0.8147\n",
      "Epoch 22 - Training Loss: 0.9339 - Valid Loss: 0.7960 - New Best\n",
      "Epoch 23 - Training Loss: 0.9141 - Valid Loss: 0.8016\n",
      "Epoch 24 - Training Loss: 0.9117 - Valid Loss: 0.7696 - New Best\n",
      "Epoch 25 - Training Loss: 0.9057 - Valid Loss: 0.7636 - New Best\n",
      "Epoch 26 - Training Loss: 0.8984 - Valid Loss: 0.7516 - New Best\n",
      "Epoch 27 - Training Loss: 0.8837 - Valid Loss: 0.7594\n",
      "Epoch 28 - Training Loss: 0.8793 - Valid Loss: 0.7486 - New Best\n",
      "Epoch 29 - Training Loss: 0.8759 - Valid Loss: 0.7522\n",
      "Epoch 30 - Training Loss: 0.8684 - Valid Loss: 0.7477 - New Best\n",
      "Time elapsed: 0:02:11.895269\n",
      "Mean time per epoch: 0:00:04.396509\n"
     ]
    }
   ],
   "source": [
    "# Define the filename for saving the best model checkpoint\n",
    "# The checkpoint will be saved in the previously defined checkpoint_path directory\n",
    "checkpoint_fname = checkpoint_path / 'bestRNN.pt'\n",
    "\n",
    "# Train the model for 30 epochs\n",
    "# Arguments:\n",
    "# - epochs: Number of epochs to train the model\n",
    "# - model: The model to train (model_rnn)\n",
    "# - optimizer: The optimizer to use for training (Adam optimizer)\n",
    "# - criterion: The loss function to use for training (CrossEntropyLoss)\n",
    "# - train_iterator: The iterator for the training dataset\n",
    "# - valid_iterator: The iterator for the validation dataset\n",
    "# - device: The device to use for computation (e.g., 'cpu' or 'cuda')\n",
    "# - checkpoint_fname: The filename for saving the best model checkpoint\n",
    "train(30, model_rnn, optimizer, criterion, train_iter, valid_iter, device, checkpoint_fname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23442/808119334.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_rnn_inference.load_state_dict(torch.load(checkpoint_fname))\n"
     ]
    }
   ],
   "source": [
    "# Create a new instance of the NameRNN model with the same hyperparameters as the trained model\n",
    "# This ensures that the model architecture matches the one used during training\n",
    "model_rnn_inference = NameRNN(\n",
    "    hidden_size=50,            # Hidden state size of the RNN\n",
    "    embedding_dim=25,          # Dimension of the embedding vectors\n",
    "    vocab_size=len(NAME.vocab),# Vocabulary size based on the 'NAME' field\n",
    "    pad_idx=pad_idx            # Padding index for the embedding layer\n",
    ")\n",
    "\n",
    "# Load the trained model's state dictionary into the new model instance\n",
    "# This will set the model parameters to the best found during training\n",
    "model_rnn_inference.load_state_dict(torch.load(checkpoint_fname))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "# This disables dropout and other training-specific behaviors\n",
    "model_rnn_inference.eval()\n",
    "\n",
    "# Send the model to the CPU for inference\n",
    "# This ensures that the model runs on the CPU, which is typically used for inference\n",
    "model_rnn_inference = model_rnn_inference.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '0']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GENDER.vocab.itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F', 0.9952084422111511]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('joana', model_rnn_inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['M', 0.9944537281990051]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('joão', model_rnn_inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F', 0.9999452829360962]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('maria', model_rnn_inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['M', 0.9954324960708618]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('marcos', model_rnn_inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('adamis', '0') ['M', 0.8871570825576782]\n",
      "('francimauro', '0') ['M', 0.9950956106185913]\n",
      "('izalto', '0') ['M', 0.999893069267273]\n",
      "('haim', '0') ['M', 0.9748303294181824]\n",
      "('cleidinei', '0') ['M', 0.76402348279953]\n",
      "('juvam', '0') ['M', 0.9981265664100647]\n",
      "('aislan', '0') ['M', 0.9315415620803833]\n",
      "('josr', '0') ['M', 0.9899489879608154]\n",
      "('irames', '0') ['M', 0.539656937122345]\n",
      "('isaelton', '0') ['M', 0.9999747276306152]\n",
      "('reilon', '0') ['M', 0.9994834661483765]\n",
      "('nilssom', '0') ['M', 0.9999818801879883]\n",
      "('lorino', '0') ['M', 0.9989786148071289]\n",
      "('leidio', '0') ['M', 0.9997301697731018]\n",
      "('geolson', '0') ['M', 0.9999927282333374]\n",
      "('atamil', '0') ['M', 0.9715228080749512]\n",
      "('joaildo', '0') ['M', 0.9999790191650391]\n",
      "('eisenhower', '0') ['M', 0.9930551648139954]\n",
      "('laerto', '0') ['M', 0.9995064735412598]\n",
      "('matteus', '0') ['M', 0.9738569259643555]\n",
      "('levair', '0') ['M', 0.7796001434326172]\n",
      "('auires', '0') ['M', 0.6104703545570374]\n",
      "('vanjo', '0') ['M', 0.9983975291252136]\n",
      "('olario', '0') ['M', 0.9999667406082153]\n",
      "('kairom', '0') ['M', 0.9954517483711243]\n",
      "('karivaldo', '0') ['M', 0.9999799728393555]\n",
      "('albertini', '0') ['M', 0.8471675515174866]\n",
      "('makswel', '0') ['M', 0.9909189939498901]\n",
      "('valmier', '0') ['M', 0.9820735454559326]\n",
      "('magnus', '0') ['M', 0.9807838797569275]\n",
      "('gildema', '0') ['F', 0.7611883282661438]\n",
      "('osmidio', '0') ['M', 0.9999281167984009]\n",
      "('anon', '0') ['M', 0.9986304044723511]\n",
      "('ulissis', '0') ['F', 0.640757143497467]\n",
      "('jasmo', '0') ['M', 0.9811385273933411]\n",
      "('jodimar', '0') ['M', 0.9583102464675903]\n",
      "('andryo', '0') ['M', 0.9999730587005615]\n",
      "('eick', '0') ['M', 0.9732726812362671]\n",
      "('marjan', '0') ['M', 0.7906067371368408]\n",
      "('odilomar', '0') ['M', 0.9909217953681946]\n",
      "('delivaldo', '0') ['M', 0.9999858140945435]\n",
      "('luadir', '0') ['F', 0.8163028359413147]\n",
      "('raynier', '0') ['M', 0.7222821712493896]\n",
      "('zoltan', '0') ['M', 0.9962288737297058]\n",
      "('edersom', '0') ['M', 0.9999514818191528]\n",
      "('ailsson', '0') ['M', 0.9999874830245972]\n",
      "('adevilton', '0') ['M', 0.9999887943267822]\n",
      "('edilino', '0') ['M', 0.9996333122253418]\n",
      "('andrsom', '0') ['M', 0.9999375343322754]\n",
      "('bejami', '0') ['M', 0.9494335651397705]\n"
     ]
    }
   ],
   "source": [
    "# Loop through the first 50 names in the training dataset\n",
    "for i in names_train[:50]:\n",
    "    # Print the name and its predicted gender label and probability\n",
    "    # The predict function takes the name, the trained model, and the device ('cpu') as arguments\n",
    "    print(i, predict(i[0], model_rnn_inference))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary to map label indices to their corresponding gender labels\n",
    "# '1' corresponds to 'F' (Female) and '0' corresponds to 'M' (Male)\n",
    "label_mapping = {'1': 'F', '0': 'M'}\n",
    "\n",
    "# Create a list of true gender labels for the validation dataset\n",
    "# Iterate over the names in the validation dataset and map the label index to the gender label using label_mapping\n",
    "true_valid = [label_mapping[i[1]] for i in names_valid]\n",
    "\n",
    "# Create a list of predicted gender labels for the validation dataset\n",
    "# Iterate over the names in the validation dataset and use the predict function to get the predicted gender label\n",
    "# The predict function takes the name, the trained model, and the device ('cpu') as arguments\n",
    "pred_valid = [predict(i[0], model_rnn_inference)[0] for i in names_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store the names where the model's prediction is incorrect\n",
    "failed = []\n",
    "\n",
    "# Loop through the validation dataset\n",
    "for i in range(len(true_valid)):\n",
    "    # Compare the true gender label with the predicted gender label\n",
    "    if true_valid[i] != pred_valid[i]:\n",
    "        # If the labels do not match, add the name to the failed list\n",
    "        failed.append(names_valid[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('niran', '0'),\n",
       " ('nubian', '1'),\n",
       " ('geremia', '0'),\n",
       " ('hironi', '1'),\n",
       " ('arisma', '0'),\n",
       " ('hisaco', '1'),\n",
       " ('rabech', '1'),\n",
       " ('neudi', '0'),\n",
       " ('cheron', '1'),\n",
       " ('cirone', '0')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choices(failed, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "792"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(failed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9560512735142334\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           F       0.96      0.96      0.96      9855\n",
      "           M       0.95      0.95      0.95      8166\n",
      "\n",
      "    accuracy                           0.96     18021\n",
      "   macro avg       0.96      0.96      0.96     18021\n",
      "weighted avg       0.96      0.96      0.96     18021\n",
      "\n",
      "Confusion Matrix:\n",
      " [[9442  413]\n",
      " [ 379 7787]]\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: ', accuracy_score(true_valid, pred_valid))\n",
    "print(f'Classification Report:\\n {classification_report(true_valid, pred_valid)}')\n",
    "print(f'Confusion Matrix:\\n {confusion_matrix(true_valid, pred_valid)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long-Short Term Memory (LSTM) Networks: A Thorough Overview\n",
    "\n",
    "Let's get into the fascinating world of LSTM Networks.\n",
    "\n",
    "## Identifying the Dilemma: Dealing with Long-Term Dependencies\n",
    "\n",
    "In our earlier cells, we examined how Recurrent Neural Networks (RNNs) utilize the concept of state retention over successive time periods to process sequential data. Nevertheless, RNNs often encounter challenges while dealing with long-term dependencies due to complex problems such as the 'vanishing gradient'.\n",
    "\n",
    "### Unfolding the Vanishing Gradient Problem\n",
    "\n",
    "The vanishing gradient problem is a troublesome circumstance where the gradients (derivatives) of the loss function, associated with the parameters (weights and biases) of the neural network, start reducing exponentially as the number of layers in the network increases. This issue becomes particularly prevalent in RNNs because calculating the gradient involves a method known as 'backpropagation through time'.\n",
    "\n",
    "> Backpropagation is an algorithmic methodology used for training neural networks. It modifies the weights of the network in alignment with the direction of the loss function's gradient until this loss function has been minimized as much as possible.\n",
    "\n",
    "The root cause behind the vanishing gradient problem rests within the repeated multiplication of gradients through the chain of network layers. As the backpropagation process occurs, the gradient, while being propagated backward through time, is continuously multiplied by the weight matrix from each successive layer. If these weights are smaller than 1, an exponential decrease can be observed in the gradient as it passes through the network, leading to a 'vanish' or near-zero gradient.\n",
    "\n",
    "### Decoding the Exploding Gradient Phenomenon\n",
    "\n",
    "On the other side of the spectrum from the vanishing gradient, we find the exploding gradient problem. This situation arises when the gradient of the loss function, relative to the network parameters, begins to increase exponentially with the growing number of layers. Just like its counterpart, this problem is quite common in RNNs, given that the gradient computation involves backpropagation through time.\n",
    "\n",
    "The root cause of the exploding gradient problem mirrors its vanishing equivalent — it's also linked to the continuous multiplication of gradients across the network layers. Here, if the weight matrix is larger than 1, the gradient may inflate exponentially as it traverses back through the network during backpropagation, leading to an 'exploding' or excessively large gradient.\n",
    "\n",
    "### Clarifying the Concept of Long-Term Dependencies\n",
    "\n",
    "These challenging vanishing and exploding gradient issues present notable obstacles for RNNs aiming to capture long-term dependencies. Long-term dependencies are inter-relationships between elements in a sequence that are separated by substantial distances. Let's consider the following sentence:\n",
    "\n",
    "> \"O cachorro passou o dia brincando .......... estava cansado.\"\n",
    "\n",
    "In this case, the word \"estava\" relies on the word \"cachorro\", even though they are significantly apart in the sentence. Unfortunately, due to the vanishing and exploding gradient problems, RNNs often fail to effectively capture these kinds of long-term dependencies.\n",
    "\n",
    "## Introduction to Long-Short Term Memory (LSTM) Networks\n",
    "\n",
    "This is where Long-Short Term Memory (LSTM) networks come into play. Being a special type of RNN, LSTM networks are adept at successfully capturing long-term dependencies. This unique capability can be traced back to [the introduction of LSTM networks in 1997 by Hochreiter and Schmidhuber](https://www.bioinf.jku.at/publications/older/2604.pdf). Since then, they have found widespread use across diverse areas such as speech recognition, language modeling, and machine translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discovering the LSTM Network Architecture\n",
    "\n",
    "An LSTM network consists of a specialized 'cell' accompanied by three regulating 'gates'—an input gate, an output gate, and a forget gate. The role of the cell is to preserve the state of the network across time, while the gates control the flow of information in and out of this cell.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"images/lstm.jpeg\" alt=\"\" style=\"width: 50%; height: 50%\"/>\n",
    "</p>\n",
    "\n",
    "\n",
    "#### Components of the LSTM Cell:\n",
    "\n",
    "##### Input Data\n",
    "These are the input data at time $( t )$, feeding into the LSTM cell.\n",
    "\n",
    "##### Hidden State\n",
    "This is the hidden state vector that carries information over time and is updated at each time step.\n",
    "\n",
    "1. **Compute Hidden State**: <br>\n",
    "\n",
    "The formula for computing the new hidden state is: $$ h_t = o_t \\cdot \\tanh(C_t) $$\n",
    "\n",
    "2. **Components:**\n",
    "- $ h_t $: new hidden state at time $ t $.\n",
    "- $ o_t $: output vector at time $ t $.\n",
    "- $ \\tanh $: tanh function, which limits values between -1 and 1.\n",
    "- $ C_t $: new cell state at time $ t $.\n",
    "\n",
    "3. **Function:**\n",
    "- The new hidden state is obtained by combining the output vector with the tanh of the new cell state, allowing the processed information to be passed to the next time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Cell State\n",
    "The thick black line running along the top of the cell. It is the primary \"memory\" mechanism of the LSTM and can carry information over many time steps.\n",
    "\n",
    "1. **Candidate Cell State**: <br>\n",
    "\n",
    "The formula for the new candidate information is: $$ \\tilde{C_t} = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C) $$\n",
    "\n",
    "2. **Components:**\n",
    "- $ \\tilde{C_t} $: new candidate information vector at time $ t $.\n",
    "- $ \\tanh $: tanh function, which limits values between -1 and 1.\n",
    "- $ W_C $: weight matrix for the new candidate information.\n",
    "- $ h_{t-1} $: hidden state of the cell at time $ t-1 $.\n",
    "- $ x_t $: input at time $ t $.\n",
    "- $ b_C $: bias vector for the new candidate information.\n",
    "\n",
    "3. **Function:**\n",
    "- This formula computes the new candidate information that might be stored in the cell state. The tanh function produces a vector of values between -1 and 1, representing the possible new information to be stored.\n",
    "\n",
    "4. **Update Cell State**:<br>\n",
    "\n",
    "The formula for updating the cell state is: $$ C_t = f_t \\cdot C_{t-1} + i_t \\cdot \\tilde{C_t} $$\n",
    "\n",
    "5. **Components:**\n",
    "- $ C_t $: new cell state at time $ t $.\n",
    "- $ f_t $: forget vector at time $ t $.\n",
    "- $ C_{t-1} $: cell state at time $ t-1 $.\n",
    "- $ i_t $: input vector at time $ t $.\n",
    "- $ \\tilde{C_t} $: new candidate information vector at time $ t $.\n",
    "\n",
    "6. **Function:**\n",
    "- The new cell state is a weighted combination of the previous cell state (modified by the forget gate) and the new information (modified by the input gate)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Input Gate\n",
    "\n",
    "The input gate regulates the inflow of information from the input layer to the cell. It consists of a sigmoid activation layer adjoined by a pointwise multiplication operation. The sigmoid function returns values between zero and one, which are multiplied with the network's input. If the sigmoid function outputs a zero, it blocks all incoming information. In contrast, an output of one allows all information to enter the cell. Thus, the input gate acts as a guard, protecting the cell from irrelevant or noise data.\n",
    "\n",
    "The formula for the input gate is:\n",
    "$$ i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i) $$\n",
    "\n",
    "**Components:**\n",
    "- $ i_t $: input vector at time $ t $.\n",
    "- $ \\sigma $: sigmoid function, which limits values between 0 and 1.\n",
    "- $ W_i $: weight matrix for the input gate.\n",
    "- $ h_{t-1} $: hidden state of the cell at time $ t-1 $.\n",
    "- $ x_t $: input at time $ t $.\n",
    "- $ b_i $: bias vector for the input gate.\n",
    "\n",
    "**Function:**\n",
    "- This gate decides which new information will be stored in the cell state. The sigmoid function produces a vector of values between 0 and 1, where values close to 0 indicate ignoring and values close to 1 indicate storing the information.\n",
    "\n",
    "\n",
    "#### The Output Gate\n",
    "\n",
    "Like its input counterpart, the output gate controls the flow of information but from the cell to the hidden state, also comprising a sigmoid activation layer and a pointwise multiplication operation. The output gate uses the returned value from the sigmoid function (between zero and one) to regulate the amount of information flowing to the hidden state—zero halts all information flow, while one permits full information transfer. This mechanism enables the output gate to secure the hidden state from receiving any unnecessary or misleading information.\n",
    "\n",
    "The formula for the output gate is:\n",
    "$$ o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o) $$\n",
    "\n",
    "**Components:**\n",
    "- $ o_t $: output vector at time $ t $.\n",
    "- $ \\sigma $: sigmoid function, which limits values between 0 and 1.\n",
    "- $ W_o $: weight matrix for the output gate.\n",
    "- $ h_{t-1} $: hidden state of the cell at time $ t-1 $.\n",
    "- $ x_t $: input at time $ t $.\n",
    "- $ b_o $: bias vector for the output gate.\n",
    "\n",
    "**Function:**\n",
    "- This gate decides which information from the cell state will be used to compute the next hidden state.\n",
    "\n",
    "\n",
    "#### The Forget Gate\n",
    "\n",
    "Finally, the forget gate manages how much information should be discarded from the cell. It comprises a sigmoid activation layer and pointwise multiplication operation. Here, the numbers generated by the sigmoid function, ranging from zero to one, decide how much information from the cell state should be kept, with zero forgetting all information and one maintaining all information. This forget gate acts as a protective barrier, helping the cell dispose of any outdated or irrelevant information that could potentially hamper the overall learning process.\n",
    "\n",
    "The formula for the forget gate is: $$ f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f) $$\n",
    "\n",
    "**Components:**\n",
    "- $ f_t $: forget vector at time $ t $.\n",
    "- $ \\sigma $: sigmoid function, which limits values between 0 and 1.\n",
    "- $ W_f $: weight matrix for the forget gate.\n",
    "- $ h_{t-1} $: hidden state of the cell at time $ t-1 $.\n",
    "- $ x_t $: input at time $ t $.\n",
    "- $ b_f $: bias vector for the forget gate.\n",
    "\n",
    "**Function:**\n",
    "- This gate decides which information from the previous cell state ($ C_{t-1} $) should be forgotten. The sigmoid function produces a vector of values between 0 and 1, where values close to 0 indicate forgetting and values close to 1 indicate keeping the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameLSTM(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding_dim, vocab_size, pad_idx):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define an embedding layer to convert our letters to vectors\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        \n",
    "        # Define an LSTM layer to encode our sequence of letters\n",
    "        self.encoder = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, num_layers=2, dropout=0.3, bidirectional=False)\n",
    "\n",
    "        # Define a dropout layer to prevent overfitting\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        # Define a classifier layer to output our final prediction\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2), # input size is the hidden size\n",
    "            self.dropout,\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 2, 2) # output size is 2 because we're predicting binary gender\n",
    "        )\n",
    "\n",
    "    def forward(self, seq, name_len):\n",
    "        # Convert our sequence of letters to a sequence of vectors using the embedding layer\n",
    "        embedded = self.embedding(seq)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        # Encode the embedded sequence using the LSTM layer\n",
    "        packed_output, (hidden, cell) = self.encoder(embedded)\n",
    "        \n",
    "        # Pass the final hidden state through the classifier layer to get our prediction\n",
    "        preds = self.classifier(hidden[-1])\n",
    "        \n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 37,827 trainable parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training Loss: 2.0513 - Valid Loss: 1.3478 - New Best\n",
      "Epoch 2 - Training Loss: 1.4648 - Valid Loss: 1.2067 - New Best\n",
      "Epoch 3 - Training Loss: 1.3246 - Valid Loss: 1.0500 - New Best\n",
      "Epoch 4 - Training Loss: 1.2106 - Valid Loss: 1.0675\n",
      "Epoch 5 - Training Loss: 1.1522 - Valid Loss: 0.9960 - New Best\n",
      "Epoch 6 - Training Loss: 1.0795 - Valid Loss: 0.9147 - New Best\n",
      "Epoch 7 - Training Loss: 1.0326 - Valid Loss: 0.8470 - New Best\n",
      "Epoch 8 - Training Loss: 0.9871 - Valid Loss: 0.8467 - New Best\n",
      "Epoch 9 - Training Loss: 0.9631 - Valid Loss: 0.8202 - New Best\n",
      "Epoch 10 - Training Loss: 0.9260 - Valid Loss: 0.7860 - New Best\n",
      "Epoch 11 - Training Loss: 0.8951 - Valid Loss: 0.7780 - New Best\n",
      "Epoch 12 - Training Loss: 0.8781 - Valid Loss: 0.7749 - New Best\n",
      "Epoch 13 - Training Loss: 0.8523 - Valid Loss: 0.7712 - New Best\n",
      "Epoch 14 - Training Loss: 0.8340 - Valid Loss: 0.7270 - New Best\n",
      "Epoch 15 - Training Loss: 0.7998 - Valid Loss: 0.7192 - New Best\n",
      "Epoch 16 - Training Loss: 0.7958 - Valid Loss: 0.6986 - New Best\n",
      "Epoch 17 - Training Loss: 0.7737 - Valid Loss: 0.7034\n",
      "Epoch 18 - Training Loss: 0.7609 - Valid Loss: 0.6635 - New Best\n",
      "Epoch 19 - Training Loss: 0.7492 - Valid Loss: 0.6643\n",
      "Epoch 20 - Training Loss: 0.7265 - Valid Loss: 0.6581 - New Best\n",
      "Epoch 21 - Training Loss: 0.7135 - Valid Loss: 0.6546 - New Best\n",
      "Epoch 22 - Training Loss: 0.7086 - Valid Loss: 0.6389 - New Best\n",
      "Epoch 23 - Training Loss: 0.6880 - Valid Loss: 0.6442\n",
      "Epoch 24 - Training Loss: 0.6737 - Valid Loss: 0.6244 - New Best\n",
      "Epoch 25 - Training Loss: 0.6599 - Valid Loss: 0.6202 - New Best\n",
      "Epoch 26 - Training Loss: 0.6652 - Valid Loss: 0.6067 - New Best\n",
      "Epoch 27 - Training Loss: 0.6473 - Valid Loss: 0.6025 - New Best\n",
      "Epoch 28 - Training Loss: 0.6335 - Valid Loss: 0.6069\n",
      "Epoch 29 - Training Loss: 0.6256 - Valid Loss: 0.5840 - New Best\n",
      "Epoch 30 - Training Loss: 0.6189 - Valid Loss: 0.5779 - New Best\n",
      "Time elapsed: 0:05:43.746877\n",
      "Mean time per epoch: 0:00:11.458229\n"
     ]
    }
   ],
   "source": [
    "# Define the filename for saving the best LSTM model checkpoint\n",
    "checkpoint_fname = checkpoint_path / 'bestLSTM.pt'\n",
    "\n",
    "# Create an instance of the NameLSTM model with specified hyperparameters\n",
    "model_lstm = NameLSTM(\n",
    "    hidden_size=50,            # Hidden state size of the LSTM\n",
    "    embedding_dim=25,          # Dimension of the embedding vectors\n",
    "    vocab_size=len(NAME.vocab),# Vocabulary size based on the 'NAME' field\n",
    "    pad_idx=pad_idx            # Padding index for the embedding layer\n",
    ")\n",
    "\n",
    "# Define the optimizer to use for training the model\n",
    "# Here, we use the Adam optimizer with a learning rate of 3e-4\n",
    "optimizer = optim.Adam(model_lstm.parameters(), lr=3e-4)\n",
    "\n",
    "# Define the loss function to use for training the model\n",
    "# Here, we use the CrossEntropyLoss, which is suitable for classification tasks\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model for 30 epochs\n",
    "# Arguments:\n",
    "# - epochs: Number of epochs to train the model\n",
    "# - model: The model to train (model_lstm)\n",
    "# - optimizer: The optimizer to use for training (Adam optimizer)\n",
    "# - criterion: The loss function to use for training (CrossEntropyLoss)\n",
    "# - train_iterator: The iterator for the training dataset\n",
    "# - valid_iterator: The iterator for the validation dataset\n",
    "# - device: The device to use for computation (e.g., 'cpu' or 'cuda')\n",
    "# - checkpoint_fname: The filename for saving the best model checkpoint\n",
    "train(30, model_lstm, optimizer, criterion, train_iter, valid_iter, device, checkpoint_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23442/798333599.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_lstm_inference.load_state_dict(torch.load(checkpoint_fname))\n"
     ]
    }
   ],
   "source": [
    "# Create a new instance of the NameLSTM model with the same hyperparameters as the trained model\n",
    "model_lstm_inference = NameLSTM(\n",
    "    hidden_size=50,            # Hidden state size of the LSTM\n",
    "    embedding_dim=25,          # Dimension of the embedding vectors\n",
    "    vocab_size=len(NAME.vocab),# Vocabulary size based on the 'NAME' field\n",
    "    pad_idx=pad_idx            # Padding index for the embedding layer\n",
    ")\n",
    "\n",
    "# Load the trained model's state dictionary into the new model instance\n",
    "# This will set the model parameters to the best found during training\n",
    "model_lstm_inference.load_state_dict(torch.load(checkpoint_fname))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "# This disables dropout and other training-specific behaviors\n",
    "model_lstm_inference.eval()\n",
    "\n",
    "# Send the model to the CPU for inference\n",
    "# This ensures that the model runs on the CPU, which is typically used for inference\n",
    "model_lstm_inference = model_lstm_inference.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {'1': 'F', '0': 'M'}  # Map label indices to gender labels ('1' -> 'F', '0' -> 'M')\n",
    "\n",
    "# Create a list of true gender labels for the validation dataset\n",
    "# Iterate over the names in the validation dataset and map the label index to the gender label using label_mapping\n",
    "true_valid = [label_mapping[i[1]] for i in names_valid]\n",
    "\n",
    "# Create a list of predicted gender labels for the validation dataset\n",
    "# Iterate over the names in the validation dataset and use the predict function to get the predicted gender label\n",
    "# The predict function takes the name, the trained model, and the device ('cpu') as arguments\n",
    "pred_valid = [predict(i[0], model_lstm_inference)[0] for i in names_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9669829643194051\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           F       0.97      0.97      0.97      9855\n",
      "           M       0.96      0.97      0.96      8166\n",
      "\n",
      "    accuracy                           0.97     18021\n",
      "   macro avg       0.97      0.97      0.97     18021\n",
      "weighted avg       0.97      0.97      0.97     18021\n",
      "\n",
      "Confusion Matrix:\n",
      " [[9542  313]\n",
      " [ 282 7884]]\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: ', accuracy_score(true_valid, pred_valid))\n",
    "print(f'Classification Report:\\n {classification_report(true_valid, pred_valid)}')\n",
    "print(f'Confusion Matrix:\\n {confusion_matrix(true_valid, pred_valid)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional Networks\n",
    "\n",
    "In previous sections, we established how LSTM networks effectively capture long-term dependencies. Nonetheless, traditional LSTM networks possess one built-in limitation - they are unidirectional, meaning they only consider past information while making predictions about the future. This quality is valuable in numerous instances, but may not suffice in certain scenarios.\n",
    "\n",
    "Consider the sentence in Portuguese:\n",
    "\n",
    "> \"O cachorro passou o dia brincando .......... estava cansado.\"\n",
    "\n",
    "In this context, the word \"estava\" (was) depends on both the preceding word \"cachorro\" (dog), as well as the subsequent word \"cansado\" (tired). Given that a standard LSTM network only takes past information into account, it would struggle to capture the long-term dependency between \"estava\" and \"cansado\".\n",
    "\n",
    "To circumvent this issue, we introduce a specialized type of architecture called a bidirectional LSTM network.\n",
    "\n",
    "### Understanding Bidirectional LSTM Networks\n",
    "\n",
    "A bidirectional LSTM network comprises two separate LSTM layers, each processing data sequences in opposing directions—one from the past towards the future, and the other from the future backwards to the past. The outputs from these two LSTM layers are then amalgamated to generate the final output.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"images/bidirectional_lstm.webp\" alt=\"\" style=\"width: 50%; height: 50%\"/>\n",
    "</p>\n",
    "\n",
    "While we have been focusing on LSTM networks, it's important to clarify that bidirectionality is not exclusive to LSTMs. In fact, all types of recurrent networks can be configured to work in a bidirectional manner.\n",
    "\n",
    "### Comprehending the Dual Processing Mechanism\n",
    "\n",
    "A bidirectional network can be visualized as two distinct networks operating concurrently, each processing sequences in different directions.\n",
    "\n",
    "One part of the network processes sequences from the past to the future, thus capturing the past dependencies just like a regular LSTM. This 'forward' layer scans the input sequence in the natural order, from the first element to the last, unearthing any dependencies that look ahead.\n",
    "\n",
    "The other part, a 'backward' layer, processes sequences from future to past, enabling it to take future information into account. In essence, this layer reads the input sequence backwards, starting from the last element and moving to the first one, capturing dependencies that look backward.\n",
    "\n",
    "Upon completion of processing, the outputs from these two parts of the network are combined—often concatenated or added—to produce the final output. This dual nature imparts bidirectional networks with their unique ability to perceive patterns considering both past and future contexts, rendering them particularly useful in tasks such as language translation, text generation, speech recognition, where understanding the full context is crucial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameLSTMBidir(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding_dim, vocab_size, pad_idx):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define an embedding layer to convert our letters to vectors\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        \n",
    "        # Define a bidirectional LSTM layer to encode our sequence of letters\n",
    "        self.encoder = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, num_layers=2, dropout=0.3, bidirectional=True)\n",
    "\n",
    "        # Define a dropout layer to prevent overfitting\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        # Define a classifier layer to output our final prediction\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size // 2), # input size is the hidden size times 2 because of bidirectionality\n",
    "            self.dropout,\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 2, 2) # output size is 2 because we're predicting binary gender\n",
    "        )\n",
    "\n",
    "    def forward(self, seq, name_len):\n",
    "        # Convert our sequence of letters to a sequence of vectors using the embedding layer\n",
    "        embedded = self.embedding(seq)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        # Encode the embedded sequence using the bidirectional LSTM layer\n",
    "        packed_output, (hidden, cell) = self.encoder(embedded)\n",
    "        \n",
    "        # Concatenate the final hidden states from both directions\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        \n",
    "        # Pass the concatenated hidden state through the classifier layer to get our prediction\n",
    "        preds = self.classifier(hidden)\n",
    "        \n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 94,877 trainable parameters\n",
      "Epoch 1 - Training Loss: 1.8746 - Valid Loss: 1.3111 - New Best\n",
      "Epoch 2 - Training Loss: 1.3256 - Valid Loss: 1.0331 - New Best\n",
      "Epoch 3 - Training Loss: 1.1756 - Valid Loss: 0.9460 - New Best\n",
      "Epoch 4 - Training Loss: 1.0809 - Valid Loss: 0.9016 - New Best\n",
      "Epoch 5 - Training Loss: 1.0114 - Valid Loss: 0.8668 - New Best\n",
      "Epoch 6 - Training Loss: 0.9444 - Valid Loss: 0.8015 - New Best\n",
      "Epoch 7 - Training Loss: 0.8902 - Valid Loss: 0.8120\n",
      "Epoch 8 - Training Loss: 0.8422 - Valid Loss: 0.7544 - New Best\n",
      "Epoch 9 - Training Loss: 0.8007 - Valid Loss: 0.7071 - New Best\n",
      "Epoch 10 - Training Loss: 0.7699 - Valid Loss: 0.7160\n",
      "Epoch 11 - Training Loss: 0.7373 - Valid Loss: 0.6608 - New Best\n",
      "Epoch 12 - Training Loss: 0.7142 - Valid Loss: 0.6514 - New Best\n",
      "Epoch 13 - Training Loss: 0.6893 - Valid Loss: 0.6241 - New Best\n",
      "Epoch 14 - Training Loss: 0.6655 - Valid Loss: 0.6308\n",
      "Epoch 15 - Training Loss: 0.6431 - Valid Loss: 0.6001 - New Best\n",
      "Epoch 16 - Training Loss: 0.6147 - Valid Loss: 0.5734 - New Best\n",
      "Epoch 17 - Training Loss: 0.5975 - Valid Loss: 0.5742\n",
      "Epoch 18 - Training Loss: 0.5700 - Valid Loss: 0.5715 - New Best\n",
      "Epoch 19 - Training Loss: 0.5624 - Valid Loss: 0.5603 - New Best\n",
      "Epoch 20 - Training Loss: 0.5535 - Valid Loss: 0.5695\n",
      "Epoch 21 - Training Loss: 0.5305 - Valid Loss: 0.5236 - New Best\n",
      "Epoch 22 - Training Loss: 0.5213 - Valid Loss: 0.5361\n",
      "Epoch 23 - Training Loss: 0.4993 - Valid Loss: 0.5316\n",
      "Epoch 24 - Training Loss: 0.4852 - Valid Loss: 0.5043 - New Best\n",
      "Epoch 25 - Training Loss: 0.4766 - Valid Loss: 0.5142\n",
      "Epoch 26 - Training Loss: 0.4635 - Valid Loss: 0.4932 - New Best\n",
      "Epoch 27 - Training Loss: 0.4574 - Valid Loss: 0.4838 - New Best\n",
      "Epoch 28 - Training Loss: 0.4397 - Valid Loss: 0.4954\n",
      "Epoch 29 - Training Loss: 0.4304 - Valid Loss: 0.4807 - New Best\n",
      "Epoch 30 - Training Loss: 0.4207 - Valid Loss: 0.4770 - New Best\n",
      "Time elapsed: 0:08:32.219350\n",
      "Mean time per epoch: 0:00:17.073978\n"
     ]
    }
   ],
   "source": [
    "# Define the filename for saving the best bidirectional LSTM model checkpoint\n",
    "checkpoint_fname = checkpoint_path / 'bestLSTMbidir.pt'\n",
    "\n",
    "# Create an instance of the NameLSTMBidir model with specified hyperparameters\n",
    "model_lstm_bidir = NameLSTMBidir(\n",
    "\thidden_size=50,            # Hidden state size of the LSTM\n",
    "\tembedding_dim=25,          # Dimension of the embedding vectors\n",
    "\tvocab_size=len(NAME.vocab),# Vocabulary size based on the 'NAME' field\n",
    "\tpad_idx=pad_idx            # Padding index for the embedding layer\n",
    ")\n",
    "\n",
    "# Define the optimizer to use for training the model\n",
    "# Here, we use the Adam optimizer with a learning rate of 3e-4\n",
    "optimizer = optim.Adam(model_lstm_bidir.parameters(), lr=3e-4)\n",
    "\n",
    "# Define the loss function to use for training the model\n",
    "# Here, we use the CrossEntropyLoss, which is suitable for classification tasks\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model for 30 epochs\n",
    "# Arguments:\n",
    "# - epochs: Number of epochs to train the model\n",
    "# - model: The model to train (model_lstm_bidir)\n",
    "# - optimizer: The optimizer to use for training (Adam optimizer)\n",
    "# - criterion: The loss function to use for training (CrossEntropyLoss)\n",
    "# - train_iterator: The iterator for the training dataset\n",
    "# - valid_iterator: The iterator for the validation dataset\n",
    "# - device: The device to use for computation (e.g., 'cpu' or 'cuda')\n",
    "# - checkpoint_fname: The filename for saving the best model checkpoint\n",
    "train(30, model_lstm_bidir, optimizer, criterion, train_iter, valid_iter, device, checkpoint_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23442/648980640.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_lstm_bidir_inference.load_state_dict(torch.load(checkpoint_fname))\n"
     ]
    }
   ],
   "source": [
    "# Create a new instance of the NameLSTMBidir model with the same hyperparameters as the trained model\n",
    "model_lstm_bidir_inference = NameLSTMBidir(\n",
    "    hidden_size=50,            # Hidden state size of the LSTM\n",
    "    embedding_dim=25,          # Dimension of the embedding vectors\n",
    "    vocab_size=len(NAME.vocab),# Vocabulary size based on the 'NAME' field\n",
    "    pad_idx=pad_idx            # Padding index for the embedding layer\n",
    ")\n",
    "\n",
    "# Load the trained model's state dictionary into the new model instance\n",
    "# This will set the model parameters to the best found during training\n",
    "model_lstm_bidir_inference.load_state_dict(torch.load(checkpoint_fname))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "# This disables dropout and other training-specific behaviors\n",
    "model_lstm_bidir_inference.eval()\n",
    "\n",
    "# Send the model to the CPU for inference\n",
    "# This ensures that the model runs on the CPU, which is typically used for inference\n",
    "model_lstm_bidir_inference = model_lstm_bidir_inference.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {'1': 'F', '0': 'M'}  # Map label indices to gender labels ('1' -> 'F', '0' -> 'M')\n",
    "\n",
    "# Create a list of true gender labels for the validation dataset\n",
    "# Iterate over the names in the validation dataset and map the label index to the gender label using label_mapping\n",
    "true_valid = [label_mapping[i[1]] for i in names_valid]\n",
    "\n",
    "# Create a list of predicted gender labels for the validation dataset\n",
    "# Iterate over the names in the validation dataset and use the predict function to get the predicted gender label\n",
    "# The predict function takes the name, the trained bidirectional LSTM model, and the device ('cpu') as arguments\n",
    "pred_valid = [predict(i[0], model_lstm_bidir_inference)[0] for i in names_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.974862660229732\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           F       0.97      0.98      0.98      9855\n",
      "           M       0.98      0.97      0.97      8166\n",
      "\n",
      "    accuracy                           0.97     18021\n",
      "   macro avg       0.97      0.97      0.97     18021\n",
      "weighted avg       0.97      0.97      0.97     18021\n",
      "\n",
      "Confusion Matrix:\n",
      " [[9662  193]\n",
      " [ 260 7906]]\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: ', accuracy_score(true_valid, pred_valid))\n",
    "print(f'Classification Report:\\n {classification_report(true_valid, pred_valid)}')\n",
    "print(f'Confusion Matrix:\\n {confusion_matrix(true_valid, pred_valid)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gated Recurrent Unit (GRU) Networks\n",
    "\n",
    "Just as we have explored LSTM networks, another prominent variation of RNNs that has seen widespread adoption due to its efficient handling of long-term dependencies is the Gated Recurrent Unit (GRU).\n",
    "\n",
    "While LSTMs effectively address the limitations of traditional RNNs by tackling the vanishing and exploding gradient problems, they come with a complex architecture that can be computationally challenging to deal with. This complexity primarily stems from the need to calculate and store three different kinds of gates (input, output, forget) at every time step.\n",
    "\n",
    "To reconcile this, GRUs were introduced in 2014 by Kyunghyun Cho et al. in their paper [\"Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation\"](https://arxiv.org/abs/1406.1078).\n",
    "\n",
    "The GRU architecture simplifies LSTM by merging the cell state and hidden state into a single entity known as the hidden state and employing only two types of gating units: update and reset gate. This reduction in complexity results in fewer calculations and memory requirements while maintaining competitive performance.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"images/gru.png\" alt=\"\" style=\"width: 50%; height: 50%\"/>\n",
    "</p>\n",
    "\n",
    "### The Hidden State\n",
    "\n",
    "In GRUs, there is no explicit cell state like in LSTMs. Instead, the hidden state alone stores the network's historical information from previous time steps. This economical design allows for efficient memory management and computational savings.\n",
    "\n",
    "### The Update Gate\n",
    "\n",
    "The update gate essentially governs how much of the past information should be kept or 'updated'. It takes the current input and previous hidden state, applies some transformations, and generates an output between zero and one. If the update gate outputs zero, the GRU disregards all past information and writes entirely new information. In contrast, if the update gate outputs one, it retains all past information.\n",
    "\n",
    "### The Reset Gate\n",
    "\n",
    "The reset gate is in charge of deciding how much past information is discarded before the new input is processed. Like the update gate, the reset gate accepts the current input and previous hidden state, applies some transformations, and produces an output between zero and one. A value close to zero means 'forget a significant portion of information', while a value close to one indicates 'keep most of the information'.\n",
    "\n",
    "\n",
    "> GRUs have been found remarkably effective for numerous tasks such as machine translation, text generation, and speech recognition. They retain the strengths of LSTM networks in handling long-term dependencies while offering a more efficient, less computationally-intensive network architecture.\n",
    ">\n",
    "> However, the choice between LSTMs and GRUs largely depends on the specific task at hand. Some empirical studies suggest that while GRUs train faster and perform comparably to LSTMs on simpler tasks, LSTMs often have the edge on tasks that require more complex learning or longer sequence modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameGRUBidir(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding_dim, vocab_size, pad_idx):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define an embedding layer to convert our letters to vectors\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        \n",
    "        # Define a bidirectional GRU layer to encode our sequence of letters\n",
    "        self.encoder = nn.GRU(input_size=embedding_dim, hidden_size=hidden_size, num_layers=2, dropout=0.3, bidirectional=True)\n",
    "\n",
    "        # Define a dropout layer to prevent overfitting\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        # Define a classifier layer to output our final prediction\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size // 2), # input size is the hidden size times 2 because of bidirectionality\n",
    "            self.dropout,\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 2, 2) # output size is 2 because we're predicting binary gender\n",
    "        )\n",
    "\n",
    "    def forward(self, seq, name_len):\n",
    "        # Convert our sequence of letters to a sequence of vectors using the embedding layer\n",
    "        embedded = self.embedding(seq)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        # Encode the embedded sequence using the bidirectional GRU layer\n",
    "        packed_output, hidden = self.encoder(embedded)\n",
    "        \n",
    "        # Concatenate the final hidden states from both directions\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        \n",
    "        # Pass the concatenated hidden state through the classifier layer to get our prediction\n",
    "        preds = self.classifier(hidden)\n",
    "        \n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 71,977 trainable parameters\n",
      "Epoch 1 - Training Loss: 1.9348 - Valid Loss: 1.3102 - New Best\n",
      "Epoch 2 - Training Loss: 1.3564 - Valid Loss: 1.0898 - New Best\n",
      "Epoch 3 - Training Loss: 1.1990 - Valid Loss: 0.9930 - New Best\n",
      "Epoch 4 - Training Loss: 1.0809 - Valid Loss: 0.9091 - New Best\n",
      "Epoch 5 - Training Loss: 1.0194 - Valid Loss: 0.8808 - New Best\n",
      "Epoch 6 - Training Loss: 0.9599 - Valid Loss: 0.8324 - New Best\n",
      "Epoch 7 - Training Loss: 0.8946 - Valid Loss: 0.7953 - New Best\n",
      "Epoch 8 - Training Loss: 0.8572 - Valid Loss: 0.7326 - New Best\n",
      "Epoch 9 - Training Loss: 0.8118 - Valid Loss: 0.7091 - New Best\n",
      "Epoch 10 - Training Loss: 0.7896 - Valid Loss: 0.7419\n",
      "Epoch 11 - Training Loss: 0.7469 - Valid Loss: 0.6820 - New Best\n",
      "Epoch 12 - Training Loss: 0.7172 - Valid Loss: 0.6809 - New Best\n",
      "Epoch 13 - Training Loss: 0.6912 - Valid Loss: 0.6330 - New Best\n",
      "Epoch 14 - Training Loss: 0.6659 - Valid Loss: 0.6246 - New Best\n",
      "Epoch 15 - Training Loss: 0.6594 - Valid Loss: 0.6065 - New Best\n",
      "Epoch 16 - Training Loss: 0.6362 - Valid Loss: 0.5821 - New Best\n",
      "Epoch 17 - Training Loss: 0.6090 - Valid Loss: 0.5765 - New Best\n",
      "Epoch 18 - Training Loss: 0.5928 - Valid Loss: 0.5641 - New Best\n",
      "Epoch 19 - Training Loss: 0.5757 - Valid Loss: 0.5622 - New Best\n",
      "Epoch 20 - Training Loss: 0.5690 - Valid Loss: 0.5696\n",
      "Epoch 21 - Training Loss: 0.5561 - Valid Loss: 0.5290 - New Best\n",
      "Epoch 22 - Training Loss: 0.5419 - Valid Loss: 0.5158 - New Best\n",
      "Epoch 23 - Training Loss: 0.5219 - Valid Loss: 0.5109 - New Best\n",
      "Epoch 24 - Training Loss: 0.5068 - Valid Loss: 0.5261\n",
      "Epoch 25 - Training Loss: 0.4926 - Valid Loss: 0.5171\n",
      "Epoch 26 - Training Loss: 0.4870 - Valid Loss: 0.5365\n",
      "Epoch 27 - Training Loss: 0.4771 - Valid Loss: 0.5372\n",
      "Epoch 28 - Training Loss: 0.4707 - Valid Loss: 0.5420\n",
      "Epoch 29 - Training Loss: 0.4634 - Valid Loss: 0.5196\n",
      "Epoch 30 - Training Loss: 0.4503 - Valid Loss: 0.5082 - New Best\n",
      "Time elapsed: 0:08:33.343148\n",
      "Mean time per epoch: 0:00:17.111438\n"
     ]
    }
   ],
   "source": [
    "# Define the filename for saving the best bidirectional GRU model checkpoint\n",
    "checkpoint_fname = checkpoint_path / 'bestGRUBidir.pt'\n",
    "\n",
    "# Create an instance of the NameGRUBidir model with specified hyperparameters\n",
    "model_gru_bidir = NameGRUBidir(\n",
    "    hidden_size=50,            # Hidden state size of the GRU\n",
    "    embedding_dim=25,          # Dimension of the embedding vectors\n",
    "    vocab_size=len(NAME.vocab),# Vocabulary size based on the 'NAME' field\n",
    "    pad_idx=pad_idx            # Padding index for the embedding layer\n",
    ")\n",
    "\n",
    "# Define the optimizer to use for training the model\n",
    "# Here, we use the Adam optimizer with a learning rate of 3e-4\n",
    "optimizer = optim.Adam(model_gru_bidir.parameters(), lr=3e-4)\n",
    "\n",
    "# Define the loss function to use for training the model\n",
    "# Here, we use the CrossEntropyLoss, which is suitable for classification tasks\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model for 30 epochs\n",
    "# Arguments:\n",
    "# - epochs: Number of epochs to train the model\n",
    "# - model: The model to train (model_gru_bidir)\n",
    "# - optimizer: The optimizer to use for training (Adam optimizer)\n",
    "# - criterion: The loss function to use for training (CrossEntropyLoss)\n",
    "# - train_iterator: The iterator for the training dataset\n",
    "# - valid_iterator: The iterator for the validation dataset\n",
    "# - device: The device to use for computation (e.g., 'cpu' or 'cuda')\n",
    "# - checkpoint_fname: The filename for saving the best model checkpoint\n",
    "train(30, model_gru_bidir, optimizer, criterion, train_iter, valid_iter, device, checkpoint_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23442/2645360135.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_gru_bidir_inference.load_state_dict(torch.load(checkpoint_fname))\n"
     ]
    }
   ],
   "source": [
    "# Create a new instance of the NameGRUBidir model with the same hyperparameters as the trained model\n",
    "# This ensures that the model architecture matches the one used during training\n",
    "model_gru_bidir_inference = NameGRUBidir(\n",
    "    hidden_size=50,            # Hidden state size of the GRU\n",
    "    embedding_dim=25,          # Dimension of the embedding vectors\n",
    "    vocab_size=len(NAME.vocab),# Vocabulary size based on the 'NAME' field\n",
    "    pad_idx=pad_idx            # Padding index for the embedding layer\n",
    ")\n",
    "\n",
    "# Load the trained model's state dictionary into the new model instance\n",
    "# This will set the model parameters to the best found during training\n",
    "model_gru_bidir_inference.load_state_dict(torch.load(checkpoint_fname))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "# This disables dropout and other training-specific behaviors\n",
    "model_gru_bidir_inference.eval()\n",
    "\n",
    "# Send the model to the CPU for inference\n",
    "# This ensures that the model runs on the CPU, which is typically used for inference\n",
    "model_gru_bidir_inference = model_gru_bidir_inference.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {'1': 'F', '0': 'M'}  # Map label indices to gender labels ('1' -> 'F', '0' -> 'M')\n",
    "\n",
    "# Create a list of true gender labels for the validation dataset\n",
    "# Iterate over the names in the validation dataset and map the label index to the gender label using label_mapping\n",
    "true_valid = [label_mapping[i[1]] for i in names_valid]\n",
    "\n",
    "# Create a list of predicted gender labels for the validation dataset\n",
    "# Iterate over the names in the validation dataset and use the predict function to get the predicted gender label\n",
    "# The predict function takes the name, the trained bidirectional GRU model, and the device ('cpu') as arguments\n",
    "pred_valid = [predict(i[0], model_gru_bidir_inference)[0] for i in names_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9731979357416347\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           F       0.97      0.98      0.98      9855\n",
      "           M       0.98      0.96      0.97      8166\n",
      "\n",
      "    accuracy                           0.97     18021\n",
      "   macro avg       0.97      0.97      0.97     18021\n",
      "weighted avg       0.97      0.97      0.97     18021\n",
      "\n",
      "Confusion Matrix:\n",
      " [[9691  164]\n",
      " [ 319 7847]]\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: ', accuracy_score(true_valid, pred_valid))\n",
    "print(f'Classification Report:\\n {classification_report(true_valid, pred_valid)}')\n",
    "print(f'Confusion Matrix:\\n {confusion_matrix(true_valid, pred_valid)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison\n",
    "\n",
    "| Architecture | Accuracy | Number of Parameters |\n",
    "|-------------------------|----------|----------------------|\n",
    "| Vanilla RNN | 95.57% | 10,977 |\n",
    "| LSTM | 96.61% | 37,827 |\n",
    "| Bidirectional LSTM | 97.37% | 94,877 |\n",
    "| Bidirectional GRU | 97.30% | 71,977 |\n",
    "\n",
    "## Analysis\n",
    "\n",
    "- **Accuracy**:\n",
    "- The **Bidirectional LSTM** model achieves the highest accuracy at 97.37%, followed closely by the **Bidirectional GRU** at 97.28%.\n",
    "- The **LSTM** model outperforms the **Vanilla RNN**, with an accuracy of 96.61% compared to 95.57%.\n",
    "\n",
    "- **Number of Parameters**:\n",
    "- The **Vanilla RNN** has the fewest parameters (10,977), making it the most lightweight model.\n",
    "- The **Bidirectional LSTM** has the highest number of parameters (94,877), indicating a more complex and potentially more powerful model.\n",
    "- The **Bidirectional GRU** has fewer parameters (71,977) than the **Bidirectional LSTM**, but more than the **LSTM** (37,827) and **Vanilla RNN**.\n",
    "\n",
    "## Considerations\n",
    "\n",
    "- The **Bidirectional LSTM** provides the best accuracy but at the cost of having the highest number of parameters.\n",
    "- The **Bidirectional GRU** offers a slightly lower accuracy than the Bidirectional LSTM but with fewer parameters, which might be a good trade-off depending on the application.\n",
    "- The **LSTM** improves accuracy significantly over the **Vanilla RNN** without an excessive increase in parameters.\n",
    "- The **Vanilla RNN** is the simplest model with the fewest parameters but also the lowest accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrapping Up: Unleashing the Power of RNNs, LSTMs, and GRUs in NLP\n",
    "\n",
    "We have now covered an extensive journey, exploring the foundations and insights into the world of Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) networks, and Gated Recurrent Units (GRU). We've examined the strengths and limitations of RNNs and learned how LSTMs and GRUs have significantly improved upon these foundations to better handle long-term dependencies.\n",
    "\n",
    "Understanding these three forms of neural networks is crucial for anyone interested in Natural Language Processing (NLP). In NLP tasks, we often deal with sequenced data where understanding temporal dependencies is key, be it understanding the sentiment behind a customer review or translating a passage from one language to another. RNNs, LSTMs, and GRUs offer us powerful tools to make sense of this complex, sequential data.\n",
    "\n",
    "However, remember that the choice of architecture should be dictated by the specifics of your task. While RNNs can suffice for simpler, short-term dependencies, LSTMs and GRUs are better suited for more complex tasks or longer sequences. But no model can serve as a silver bullet. Experimenting, iterating, and continuous learning are part of the process.\n",
    "\n",
    "Next class we will start to work with a new type of neural network: Transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "1. What are the main types of recurrent neural networks discussed in this class?\n",
    "\n",
    "2. What is the vanishing gradient problem and how do LSTMs address it?\n",
    "\n",
    "3. How does a bidirectional LSTM network process sequences differently from a standard LSTM?\n",
    "\n",
    "4. What are the key components of an LSTM cell and what are their functions?\n",
    "\n",
    "5. How does a GRU simplify the architecture of an LSTM?\n",
    "\n",
    "6. What was the case study used to demonstrate the application of these different network architectures?\n",
    "\n",
    "7. Which model achieved the highest accuracy on the name gender classification task?\n",
    "\n",
    "8. How many trainable parameters did the bidirectional LSTM model have compared to the vanilla RNN?\n",
    "\n",
    "9. What are some of the tradeoffs to consider when choosing between RNNs, LSTMs and GRUs?\n",
    "\n",
    "10. Why are recurrent networks particularly useful for natural language processing tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Answers are commented inside this cell`\n",
    "\n",
    "<!-- 1. RNNs are the simplest, processing sequences with hidden states. LSTMs introduce a memory cell and gates to better handle long-term dependencies. GRUs simplify LSTMs by combining hidden and cell states and using only update and reset gates.\n",
    "\n",
    "2. RNNs process sequential data by maintaining a hidden state that captures information from previous time steps. They face challenges with vanishing and exploding gradients when dealing with long sequences.\n",
    "\n",
    "3. Vanishing gradients occur when gradients become extremely small during backpropagation, preventing effective learning. Exploding gradients happen when gradients become very large, leading to unstable training.\n",
    "\n",
    "4. LSTMs have a memory cell that maintains long-term information, and gates (input, output, forget) that regulate information flow, allowing them to capture long-term dependencies effectively.\n",
    "\n",
    "5. The input gate controls what new information is added to the cell state. The forget gate determines what information to discard from the cell state. The output gate decides what information from the cell state is used to compute the hidden state.\n",
    "\n",
    "6. Bidirectional LSTMs process sequences in both forward and backward directions, capturing both past and future contexts. This allows them to have a more complete understanding of the sequence.\n",
    "\n",
    "7. GRUs combine the hidden and cell states into a single hidden state and use only two gates (update and reset), making them computationally more efficient than LSTMs while still handling long-term dependencies effectively.\n",
    "\n",
    "8. The dataset used was from the 2010 Brazilian Census, containing 90,104 names (49,274 female and 40,830 male). Names that could be associated with both genders were excluded. The data was split into 80% training and 20% validation sets.\n",
    "\n",
    "9. Bidirectional networks process sequences in both forward and backward directions, allowing them to capture both past and future contexts. This additional context helps improve the performance of sequence processing tasks.\n",
    "\n",
    "10. The Bidirectional LSTM achieved the highest accuracy (97.37%), followed by the Bidirectional GRU (97.28%), LSTM (96.61%), and Vanilla RNN (95.57%). The Bidirectional LSTM had the most parameters, while the Vanilla RNN had the fewest. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "class_applied_ml_tre",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
